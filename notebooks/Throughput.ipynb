{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Throughput\n",
    "\n",
    "We can calculate the requests per second for the days of data we have. We could parse the WARC data to look at the request records. But for efficiency we can use the CDX index file and assume that every response has a corresponding request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2056"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "cdx_files = glob.glob('warcs/liveweb-*/*cdx.gz')\n",
    "len(cdx_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to need Spark to sort, since the CDX isn't ordered by time but by URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../utils')\n",
    "from warc_spark import init\n",
    "\n",
    "sc, sqlc = init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a somewhat convoluted function that reads a set of cdx_files, opens them and returns an iterator for all the timestamps in the CDX files. We will use this function with Spark in a second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import gzip\n",
    "\n",
    "def get_times(cdx_files):\n",
    "    for cdx_file in cdx_files:\n",
    "        with gzip.open(cdx_file, 'rb') as gz:\n",
    "            fh = io.BufferedReader(gz)\n",
    "            first = True\n",
    "            for line in fh.readlines():\n",
    "                # skip the first line in each file (header)\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "                parts = line.decode().split(\" \")\n",
    "                yield (parts[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Spark to read all the cdx files for 2018. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('20181025102307', 1),\n",
       " ('20181025101204', 1),\n",
       " ('20181025101209', 1),\n",
       " ('20181025101919', 1),\n",
       " ('20181025101913', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdx = sc.parallelize(cdx_files)\n",
    "times = cdx.mapPartitions(get_times)\n",
    "times.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the results by seconds and count the number of requests in that second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_times = times.combineByKey(\n",
    "    lambda r: r,\n",
    "    lambda a, b: a + b,\n",
    "    lambda a, b: a + b\n",
    ")\n",
    "\n",
    "combined_times.take(25)\n",
    "df = combined_times.toDF(['time', 'count'])\n",
    "df.write.csv('results/times', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
